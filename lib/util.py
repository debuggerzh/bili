import csv
import json
import os.path
import random
import re
from datetime import datetime
from pprint import pprint

import requests
import pandas as pd
from bs4 import BeautifulSoup as BS
from google.protobuf.json_format import MessageToJson
from requests import RequestException
import streamlit as st
from retrying import retry

from lib.dm_pb2 import DmSegMobileReply
from lib.info import date_headers, critical_cookie, user_headers, kuai_proxies, avbvid_pattern, types

__all__ = ['auto_wrap', 'show_tags', 'show_top3_comments', 'store_sessssion_state',
           'rearrange_stat', 'Crawl']


def retry_if_error(exception):
    err_classes = (KeyError, AttributeError, TypeError)
    for cls in err_classes:
        if isinstance(exception, cls):
            return True
    return False


class Crawl:
    @staticmethod
    @st.cache_data
    def crawl_save(dest_file: str, cid: int, _need_likes=False):
        """
        å…è®¸Ctrl-Cä¸­æ–­ç‚¹èµæ•°çˆ¬å–
        è‹¥å·²çˆ¬å–è¿‡ï¼Œç›´æ¥è¿”å›ï¼Œå¹¶åœ¨streamlitç¼“å­˜
        :param _need_likes: è¡¨ç¤ºéœ€è¦æ·»åŠ å¼¹å¹•ç‚¹èµæ•°ä¸€åˆ—
        :param dest_file: è¦å­˜å‚¨çš„csvæ–‡ä»¶å
        :param cid:
        :return: å·²è¯»å–csvçš„DataFrameï¼ŒæŒ‰ç‚¹èµæ•°å’Œæ—¶é—´é™åºæ’åˆ—
        """
        if not os.path.exists(dest_file):
            with open(dest_file, mode='w', encoding='utf-8', newline='') as csv_file:
                writer = csv.writer(csv_file)
                writer.writerow(['time', 'text', 'mode', 'size', 'color', 'user', 'dmid'])
                soup = Crawl.get_danmakus_from_xml(cid, True)
                for danmaku in soup.find_all('d'):
                    properties = danmaku['p'].split(',')
                    dm_time = properties[0]
                    mode = properties[1]  # 8ä»£è¡¨é«˜çº§å¼¹å¹•
                    size = properties[2]  # 25ä¸ºé»˜è®¤å€¼
                    color = properties[3]  # åè¿›åˆ¶
                    user = properties[-3]  # åå…­è¿›åˆ¶ï¼Œè¿™é‡ŒBç«™åšäº†è°ƒæ•´ï¼Œåœ¨æœ€åå¢åŠ äº†weightå‚æ•°
                    dmtext = danmaku.text
                    dmid = properties[-2]  # å¼¹å¹•ID
                    writer.writerow((dm_time, dmtext, mode, size, color, user, dmid))
        df = pd.read_csv(dest_file)

        try:
            if _need_likes:
                if 'likes' not in df.columns:
                    df['likes'] = list(map(Crawl.get_dm_likes, (cid,) * len(df), df['dmid']))
        except KeyboardInterrupt:
            df['likes'] = (0,) * len(df)
        finally:
            if 'likes' not in df.columns:
                df['likes'] = (0,) * len(df)
            df.to_csv(dest_file, index=False)
            return df.sort_values(["likes", "time"], ascending=False, inplace=False)

    @staticmethod
    def get_danmakus_from_xml(cid, raw=False):
        """

        :param cid:
        :param raw: Trueåˆ™è¿”å›BeautifulSoupå¯¹è±¡
        :return:
        """
        url = f'https://comment.bilibili.com/{cid}.xml'
        resp = requests.get(url).content.decode()
        soup = BS(resp, 'xml')
        if raw:
            return soup
        danmakus = [x.text for x in soup.find_all('d')]
        separated = '\n'.join(danmakus)
        return separated

    @staticmethod
    @retry(retry_on_exception=retry_if_error)
    def get_space_info(uid: int):
        err_code = (-401, -404)
        try:
            base_url = 'https://api.bilibili.com/x/space/wbi/acc/info'
            remain = ('name', 'sex', 'face', 'sign', 'level', 'birthday')
            from lib.info import user_headers
            resp = requests.get(base_url, params={'mid': uid}, headers=user_headers,
                                cookies=critical_cookie)
            resp_json = resp.json()
            if resp_json['code'] in err_code:
                final_dict = dict.fromkeys(remain, '')
                final_dict['school'] = None
                final_dict['level'] = -1  # è¡¨ç¤ºå·å¯„äº†
                final_dict['sex'] = 2  # è¡¨ç¤ºä¿å¯†
            else:
                data: dict = resp_json['data']
                final_dict = {k: str(v) for k, v in data.items() if k in remain}
                # icon = get_raw(data['face'])
                # todo ç›®å‰å·²çŸ¥data['school']å¯èƒ½ä¸ºNoneï¼Œæˆ–å¯èƒ½æœ‰'name'ï¼Œæˆ–'name'å¯èƒ½ä¸ºç©ºä¸²
                if 'school' in data and data['school']:
                    if 'name' in data['school']:
                        final_dict['school'] = data['school']['name']
                        if len(final_dict['school']) == 0:
                            final_dict['school'] = None
                    else:
                        final_dict['school'] = None
                else:
                    final_dict['school'] = None
        except (KeyError, AttributeError, TypeError) as e:
            print(uid, type(e), e)
            raise e
        return final_dict

    @staticmethod
    def get_gender(mid: int):
        """

        :param mid:
        :return: 0-Female, 1-Male, -1 means secret or æ¥å£è¶…é™
        """
        # time.sleep(1)  # è¯¥æ¥å£åæ‰’ä¸¥é‡ï¼ï¼ï¼QTMD
        api_url = 'https://api.bilibili.com/x/space/acc/info'
        need_proxy = random.randint(0, 1)
        if need_proxy == 1:
            proxies = kuai_proxies
        else:
            proxies = None
        try:
            resp = requests.get(api_url, params={'mid': mid, 'jsonp': 'jsonp'},
                                cookies=critical_cookie,
                                headers=user_headers,
                                proxies=proxies)
            search = re.search(r'(?<=\"sex\":\")[ä¿å¯†ç”·å¥³]+', resp.text)
            gender = search.group(0)
        except RequestException as e:
            print(e.args)
            return -1
        except AttributeError:
            return -1
        # å› APIè¿”å›ä¸å¤Ÿä¸¥è°¨ï¼Œåªå¾—ç”¨æ­£åˆ™è¿›è¡Œæå–
        # data = resp.json()
        # gender = data['data']['sex']
        if gender == 'å¥³':
            return 0
        elif gender == 'ç”·':
            return 1
        elif gender == 'ä¿å¯†':
            return -1
        print(resp.text)  # è‹¥è¿è¡Œåˆ°æ­¤ï¼Œæ˜¯æˆ‘ä¸ç†è§£çš„

    @staticmethod
    def get_dm_likes(cid: int, dmid: int):
        """

        :param cid:
        :param dmid:
        :return: å¼¹å¹•ç‚¹èµæ•°ï¼Œ-1è¡¨ç¤ºè·å–å¤±è´¥
        """
        res = requests.get(
            f"https://api.bilibili.com/x/v2/dm/thumbup/stats?oid={cid}&ids={dmid}", headers=user_headers)
        r = res.json()
        dmid = str(dmid)
        if "data" in r:
            print(r["data"][dmid]["likes"])
            return r["data"][dmid]["likes"]
        else:
            print(r)
            return -1


class Contstant:
    INTERVAL = 60
    POSITIVE = 1
    NEGATIVE = -1
    back_path = os.path.join('resources', 'btv.jpg')
    font_path = os.path.join('resources', 'vista_black.ttf')


class Util:
    import jieba
    emotion_types = ['happy', 'like', 'anger', 'sad', 'surprise', 'disgust', 'fear']
    level_types = {'most': 8, 'very': 6, 'more': 4,
                   'ish': 2, 'insufficient': 0.5, 'inverse': -1}

    all_path = os.path.join('resources', 'dicts', 'jieba_sentiment.txt')
    abbr_path = os.path.join('resources', 'abbr.txt')

    stop_words_path = os.path.join('resources', 'stop.txt')
    positive_words_path = os.path.join('resources', 'dicts', 'positive.txt')
    negative_words_path = os.path.join('resources', 'dicts', 'negative.txt')

    most_words_path = os.path.join('resources', 'dicts', 'most.txt')
    very_words_path = os.path.join('resources', 'dicts', 'very.txt')
    more_words_path = os.path.join('resources', 'dicts', 'more.txt')
    # ishè¡¨å¤§çº¦ï¼Œå¤§æ¦‚ï¼Œå·¦å³
    ish_words_path = os.path.join('resources', 'dicts', 'ish.txt')
    insufficient_words_path = os.path.join('resources', 'dicts', 'insufficient.txt')
    inverse_words_path = os.path.join('resources', 'dicts', 'inverse.txt')

    paths = [stop_words_path, positive_words_path, negative_words_path,
             most_words_path, very_words_path, more_words_path, ish_words_path,
             insufficient_words_path, inverse_words_path]

    def __init__(self):
        pattern = r'\w+(?=\.txt)'
        for path in Util.paths:
            with open(path, encoding='utf-8') as f:
                var_name = re.search(pattern, path).group(0)
                setattr(self, var_name + '_words', f.read().splitlines())
        # è¯å…¸å½•å…¥jiebaåˆ†è¯
        Util.jieba.load_userdict(Util.all_path)
        Util.jieba.load_userdict(Util.positive_words_path)
        Util.jieba.load_userdict(Util.negative_words_path)

        self.abbr_dict = {}
        with open(Util.abbr_path, encoding='utf-8') as f:
            for line in f.read().splitlines():
                k, v = line.split(':')
                self.abbr_dict[k] = v

        df = pd.read_csv(os.path.join('resources', 'dicts/multi_dimen.csv'))
        for emotion in Util.emotion_types:
            setattr(self, emotion + '_words', df[df['type'] == emotion]['word'].tolist())

    def match_adverb(self, word):
        for lev, val in Util.level_types.items():
            if word in getattr(self, lev + '_words'):
                return val
        return 1

    def match_multi(self, word):
        for emotion in Util.emotion_types:
            if word in getattr(self, emotion + '_words'):
                return emotion

    def expand(self, sentence: str):
        for pattern, whole in self.abbr_dict.items():
            sentence = re.sub(pattern, whole, sentence, flags=re.I)
        return sentence


util = Util()


@st.cache_data
def get_date_range(vid: str):
    """

    :return:
    """
    meta = get_metadata(vid)[0]
    month_url = 'https://api.bilibili.com/x/v2/dm/history/index?type=1&oid={}&month={}'
    # date_url = 'https://api.bilibili.com/x/v2/dm/history?type=1&oid={}&date={}'
    # title, cid = get_cid(vid_url)
    # data = get_metadata(url)[0]
    cid = meta['cid']
    pubdate = meta['pubdate'] if 'pubdate' in meta else meta['pub_time']
    start_month = datetime.fromtimestamp(pubdate).strftime('%Y-%m')
    month_range = pd.date_range(start_month, datetime.today(),  # MS:month start frequency
                                freq='MS').strftime("%Y-%m").tolist()
    days: list[str] = []
    for month in month_range:
        response = requests.get(url=month_url.format(cid, month), headers=date_headers,
                                cookies=critical_cookie)
        month_data = response.json().get('data')
        # pprint(month_data)
        if month_data:
            days.extend(month_data)
        else:
            st.write(response.text)
    return days


def auto_wrap(s: str, column=1):
    """
    ä¸ºé€‚åº”st.textï¼Œæ¯ä¸€å®šå­—æ•°è‡ªåŠ¨æ¢è¡Œï¼ˆæ’å…¥æ¢è¡Œç¬¦ï¼‰
    :param column: å¹³å‡åˆ†å‰²çš„åˆ†æ æ•°
    :param s: éœ€è¦è‡ªåŠ¨æ¢è¡Œçš„æ–‡æœ¬
    :return: å·²è‡ªåŠ¨æ¢è¡Œçš„æ–‡æœ¬
    """
    if not s:
        return ''
    entire = 50
    part_sentence = s.split('\n')
    cutted = []
    for sentence in part_sentence:
        current_list = re.findall(r'.{%d}' % (entire // column), sentence)
        cutted.extend(current_list)
        remain = sentence[len(current_list) * entire // column:]
        if len(remain) == 0:
            continue
        cutted.append(remain)
    return '\n'.join(cutted)


@st.cache_data
def dm_history(cid: int, date: str):
    """

    :param date: '%Y-%m-%d'
    :type cid: object
    :return: æŸå¤©çš„å¼¹å¹•åˆ—è¡¨
    """
    url_history = f'https://api.bilibili.com/x/v2/dm/web/history/seg.so?type=1&oid={cid}&date={date}'
    resp = requests.get(url_history, cookies=critical_cookie)
    dmk = DmSegMobileReply()
    dmk.ParseFromString(resp.content)
    data_dict: dict = json.loads(MessageToJson(dmk))
    # print(data_dict['elems'][0].keys())
    hist_file_path = os.path.join(os.path.dirname(st.session_state.csv),
                                  f'{cid}_{date}.csv')
    with open(hist_file_path, mode='w', encoding='utf-8', newline='') as csv_file:
        writer = csv.writer(csv_file)
        tags = ('mode', 'fontsize', 'color', 'midHash', 'content')
        writer.writerow(tags)
        try:
            for item in data_dict['elems']:
                writer.writerow([item.get(x, '') for x in tags])
        except csv.Error:
            from pprint import pprint
            pprint(item)
    # st.write(data_dict)
    words: list[str] = [x.get('content', '') for x in data_dict.get('elems', [])]
    # list(map(lambda x=None: print(x['content']), data_dict.get('elems', [])))
    '''åŒè¯­ä¸çœŸï¼Œè¯„ä»·ä¸ºååˆ†ç‚«æŠ€çš„å†™æ³•
    Noneæ˜¯xçš„é»˜è®¤å€¼ï¼ˆäº‹å®ä¸Šå–ä¸åˆ°ï¼‰ï¼Œç”¨mapæŠŠæ¯ä¸ªelemçš„contentè¾“å‡º
    mapè¿”å›è¿­ä»£å™¨ï¼Œä½¿ç”¨listå¯å…¨éƒ¨æ‰“å°'''
    return words


def load_side():
    """

    :param sentiment: Trueæ—¶ç¦ç”¨å¤é€‰æ¡†ï¼Œä»è€Œç¦ç”¨æ—¥æœŸé€‰æ‹©
    :return: dateä¸ºNoneè¡¨ç¤ºå…¨å¼¹å¹•
    """
    metadata = st.session_state.meta
    with st.sidebar:
        show_all = st.checkbox('æ˜¾ç¤ºå…¨å‰§é›†è¯äº‘', key='all')
        ep: int = st.number_input('è¯·è¾“å…¥é›†æ•°', 1, metadata['total'], disabled=show_all,
                                  key='episode')
        disable_hist = st.checkbox('ç¦ç”¨å†å²æ—¥æœŸ', value=True,
                                   key='dis_hist')
        if not disable_hist:
            date_range = get_date_range(metadata['episodes'][ep - 1]['bvid'])
            latest_date = datetime.strptime(date_range[-1], '%Y-%m-%d')
            st.date_input('Back to:', value=latest_date, key='date',
                          min_value=datetime.strptime(date_range[0], '%Y-%m-%d'),
                          max_value=latest_date,
                          )


@st.cache_data
def load_episode(_meta: dict, ep: int):
    return _meta['episodes'][ep - 1]


@st.cache_data
def get_up_info(uid):
    """

    :param uid:
    :return: å¤§ä½“å…¨é¢çš„ç”¨æˆ·æ•°æ®
    """
    final_dict = {}
    final_dict.update(Crawl.get_space_info(uid))
    # ä»£è¡¨ä½œ
    most_bvid = get_user_videos(uid)[0]['bvid']
    final_dict['most_bvid'] = most_bvid
    # top_url = 'https://api.bilibili.com/x/space/top/arc'
    # remain = ('title', 'desc', 'pic')
    # try:
    #     resp = requests.get(top_url, params={'vmid': uid}, headers=user_headers)
    #     data = resp.json()['data']
    #     final_dict.update({k: str(v) for k, v in data.items() if k in remain})
    # except AttributeError as e:
    #     print(e)

    relation_url = 'https://api.bilibili.com/x/relation/stat'
    try:
        resp = requests.get(relation_url, params={'vmid': uid}, headers=user_headers)
        data = resp.json()['data']
        final_dict['fans'] = str(data['follower'])
    except AttributeError as e:
        print(e)

    upstat_url = "https://api.bilibili.com/x/space/navnum"
    try:
        resp = requests.get(upstat_url, params={'mid': uid, 'jsonp': 'jsonp'},
                            headers=user_headers)
        data = resp.json()['data']
        v_num = data['video']  # è§†é¢‘æ•°é‡
    except (AttributeError, TypeError) as e:
        v_num = 0
        print(e)
    finally:
        final_dict['v_num'] = v_num
    return final_dict


def show_tags(aid: int = None, cid: int = None, data=None, show=False):
    url = 'https://api.bilibili.com/x/web-interface/view/detail/tag'
    from lib.info import user_headers
    if data:
        tag_data = data
    else:
        resp = requests.get(url, {'aid': aid, 'cid': cid}, headers=user_headers)
        tag_data = resp.json()['data']
    tags_len = len(tag_data)
    turns = (tags_len - 1) // 5 + 1
    tags = []
    for i in range(turns):
        columns = st.columns(5)
        for tag, col in zip(tag_data[5 * i:], columns):
            name_ = tag['tag_name']
            if show:
                col.button(name_)
            tags.append(name_)
    return '#'.join(tags)


def show_top3_comments(aid: int):
    url = 'https://api.bilibili.com/x/v2/reply/main'
    params = {'csrf': '4344d9ff5f9a262d325abfb315ea5439',
              'mode': 3,
              'oid': aid,
              'type': 1,
              }
    from lib.info import user_headers
    resp = requests.get(url, params, headers=user_headers).json()
    if 'data' not in resp:
        st.text('æ— ä»»ä½•è¯„è®ºã€‚')
        return
    replies = resp['data']['replies']
    st.header('è¯„è®ºTop3')
    tot_len = 0
    for i in range(3 if len(replies) > 3 else len(replies)):
        left, right = st.columns((9, 1))
        left.subheader(replies[i]['member']['uname'])
        right.text('ğŸ‘' + str(replies[i]['like']))
        message = replies[i]['content']['message']
        tot_len += len(message)
        st.text(auto_wrap(message))
    # print(tot_len)


def store_sessssion_state(metadata, vid_url):
    danmakus_csv = r'resources\danmakus\{}.csv'
    cid = metadata['cid']
    st.session_state.csv = danmakus_csv.format(cid)
    st.session_state.meta = metadata
    st.session_state.vid = vid_url


def rearrange_stat(stat, metric=False):
    from lib.info import types
    trans = {'view': 'æ’­æ”¾é‡', 'views': 'æ’­æ”¾é‡',
             'danmaku': 'å¼¹å¹•æ•°', 'like': 'ç‚¹èµæ•°', 'coin': 'æŠ•å¸æ•°', 'favorite': 'æ”¶è—æ•°',
             'share': 'è½¬å‘æ•°', 'reply': 'è¯„è®ºæ•°'}
    if metric:
        cols = st.columns(len(types))
        for tag, col in zip(types, cols):
            if tag == 'view' and tag not in stat:
                stat_value = stat['views']
            else:
                stat_value = stat[tag]
            if stat_value >= 9999:
                num = str(round(stat_value / 10000, 1)) + 'ä¸‡'
            else:
                num = str(stat_value)
            col.metric(trans[tag], num, help=num)
        return

    tbl_data = []
    for tag in types:

        stat_value = stat[tag]
        if stat_value >= 9999:
            num = str(round(stat_value / 10000, 1)) + 'ä¸‡'
        else:
            num = str(stat_value)
        tbl_data.append([trans[tag], num])
    frame = pd.DataFrame(tbl_data, columns=['æŒ‡æ ‡', 'æ•°é‡'])
    return frame


@st.cache_data
def convert_df(df, encoding='utf-8'):
    # IMPORTANT: Cache the conversion to prevent computation on every rerun
    return df.to_csv().encode(encoding)


@retry(retry_on_exception=lambda e: isinstance(e, KeyError))
def get_user_videos(uid):
    search_url = 'https://api.bilibili.com/x/space/arc/search'
    params = {'mid': uid, 'ps': 30, 'tid': 0, 'pn': 1, 'keyword': '',
              'order': 'click'  # æŒ‰ç‚¹å‡»é‡é™åº
              }
    resp = requests.get(search_url, params, headers=user_headers, stream=True)
    try:
        data = resp.json()['data']
    except RequestException:
        decoder = json.JSONDecoder()
        text = resp.text
        while text:
            json_data, index = decoder.raw_decode(text)
            text = text[index:].lstrip()
            if 'data' in json_data:
                data = json_data['data']
                break
    videos = data['list']['vlist']
    return videos


def get_avbvid(url):
    if "b23.tv" in url:
        r = requests.head(url, headers=user_headers)
        url = r.headers['Location']
    try:
        avbvid = avbvid_pattern.search(url).group(0)
    except AttributeError:
        return

    url = url.strip("/")
    m_obj = re.search(r"[?&]p=(\d+)", url)
    p = 0
    if m_obj:
        p = int(m_obj.group(1))
    # s_pos = url.rfind("/") + 1
    # r_pos = url.rfind("?")
    # avbvid = None
    # if r_pos == -1:
    #     avbvid = url[s_pos:]
    # else:
    #     avbvid = url[s_pos:r_pos]
    if avbvid.startswith("av") or avbvid.startswith('AV'):
        return "aid", avbvid[2:], p
    elif avbvid.startswith("bv") or avbvid.startswith('BV'):
        return "bvid", avbvid, p


@st.cache_data
def get_metadata(url):
    """

    :param url: Bç«™å•ä¸ªè§†é¢‘çš„é“¾æ¥
    :return: metadata, åˆ†pï¼ˆä¸åˆ†pè¿”å›0ï¼‰
    é“¾æ¥æ— æ³•è¯†åˆ«æ—¶ï¼Œè¿”å›None
    """
    tup = get_avbvid(url)
    if tup is None:
        return
    typ, avbvid, p = tup
    res = requests.get(
        f"https://api.bilibili.com/x/web-interface/view?{typ}={avbvid}",
        headers=user_headers)
    res.encoding = "u8"
    data: dict = res.json()['data']
    return data, p


def extract_meta(metadata):
    video_keys = ('cid', 'bvid', 'title', 'aid', 'pic',)
    extracted = {k: str(v) for k, v in metadata.items()
                 if k in video_keys}
    extracted_stat = {k: v for k, v in metadata['stat'].items()
                      if k in types}
    # extracted.update(extracted_stat)
    extracted['stat'] = extracted_stat
    extracted['desc'] = auto_wrap(metadata['desc'])
    tags = show_tags(extracted['aid'], extracted['cid'], show=False)
    extracted['tag'] = tags
    return extracted


if __name__ == '__main__':
    vid_u = "https://www.bilibili.com/video/BV1XW411F7L6/?vd_source=58a41ac877c965b4616d2d9f764c219d"
    print(get_date_range(vid_u))
    text = '''æˆ‘çœŸçš„è§‰å¾—å¾ˆææ€–ï¼Œå°±æ˜¯æœ‰ä¸€ä¸ªäººå¼€ç›’äº†æˆ‘åœ¨åˆ«çš„å¹³å°çœ‹ç›´æ’­çš„ç§äººè´¦å·ï¼Œç„¶åå¼€å°å·æŠŠè¿™ä¸ªè´¦å·å‘ç»™äº†æˆ‘çš„è€å…¬ç²‰ã€‚è·Ÿæˆ‘çš„è€å…¬ç²‰è¯´æˆ‘èƒŒåœ°é‡Œåœ¨çœ‹åˆ«çš„ç”·ä¸»æ’­ï¼Œå«ä»–ä»¬ä¸è¦å†å–œæ¬¢æˆ‘äº†ä¸è¦å†ç»™æˆ‘é€ç¤¼ç‰©äº†ã€‚
æˆ‘è§‰å¾—è¿™ä¸ªäº‹æƒ…å¾ˆææ€–å•Šï¼Œé¦–å…ˆï¼Œè¿™ä¸ªäººä»–çœŸçš„å¾ˆé˜´æš—ï¼Œå…¶æ¬¡ï¼Œä»–æ˜¯å‡ºäºä»€ä¹ˆç›®çš„ã€å‡ºäºä»€ä¹ˆèº«ä»½çš„å‘¢ï¼Ÿ
ä»–æ˜¯æˆ‘çš„ç²‰ä¸ï¼Ÿè¿˜æ˜¯æˆ‘çš„ç«äº‰å¯¹æ‰‹ï¼Ÿè‡ªå·±æ²¡æœ‰èƒ½åŠ›æ‰“å€’æˆ‘ï¼Œå°±åªèƒ½èº²åœ¨é˜´æš—çš„è§’è½é‡Œï¼Œæƒ³è¦å€Ÿåˆ€æ€äººï¼Œä¼ æˆ‘å’Œåˆ«çš„ç”·äººçš„ç»¯é—»ï¼Œç„¶ååˆ©ç”¨æˆ‘ç²‰ä¸å¯¹æˆ‘çš„çˆ±ï¼Œæƒ³è®©æˆ‘ç²‰ä¸æŠŠæˆ‘ä¸€åˆ€æ…æ­»æ˜¯å§ã€‚
çœŸçš„å¾ˆææ€–ï¼Œè¿å¤–å–éƒ½ä¸æ•¢ç‚¹äº†ï¼Œå°±æ€•æœ‰äººè¹²åœ¨æˆ‘å®¶é—¨å£ï¼Œæˆ‘ä¸€å¼€é—¨â€œä½ å°±æ˜¯æµ…å·ç‰ä¹ƒæ˜¯å§ å°±æ˜¯ä½ å·çœ‹ç”·ä¸»æ’­æ˜¯å§â€ç„¶å ç¢°ï¼ç»™æˆ‘ä¸€åˆ€å™¶äº†
å…³äºæˆ‘çœ‹è¿™ä¸ªä¸»æ’­ï¼Œæ˜¯ä»æˆ‘å°æ—¶å¼€å§‹å°±åœ¨çœ‹ï¼Œä¹‹å‰çš„è§†é¢‘é‡Œä¹Ÿæœ‰è¯´è¿‡ï¼Œä»–çœŸçš„æ˜¯ä¸€ä¸ªéå¸¸éå¸¸å¥½ï¼Œéå¸¸éå¸¸æ­£èƒ½é‡çš„äººï¼Œæˆ‘ä»ä»–èº«ä¸Šå­¦åˆ°äº†å¾ˆå¤šåšäººçš„é“ç†ã€‚è€Œä¸”ï¼Œä»–ä¹Ÿæœ‰ä¸€ä¸ªå¦¹å¦¹ï¼Œç„¶åæˆ‘æœ‰ä¸€ä¸ªå“¥å“¥ã€‚è€Œä¸”ä»–å’Œå¥¹å¦¹å¦¹çš„å¹´é¾„å·®å’Œæˆ‘å’Œæˆ‘å“¥çš„å¹´é¾„å·®å·®ä¸å¤šï¼Œä»–éœ€è¦èµšé’±å…»å¦¹å¦¹ï¼Œæˆ‘éœ€è¦èµšé’±å…»å®¶ï¼Œæ‰€ä»¥æˆ‘æ„Ÿè§‰ï¼Œä»–æ„¿æ„æ­ç†ä¸€ä¸ªæˆ‘è¿™æ ·çš„å°ç²‰ä¸ï¼Œåº”è¯¥ä¹Ÿæœ‰ä¸€äº›å…±é¸£å§ã€‚
æ‰€ä»¥çœŸçš„ä¸è¦æŠŠæ‰€æœ‰äººå¯¹ä¸»æ’­çš„å–œæ¬¢å’Œæ”¯æŒéƒ½æƒ³å¾—é‚£ä¹ˆè‚®è„ï¼Œä¸æ˜¯æ‰€æœ‰äººç»™ä¸»æ’­é€ç¤¼ç‰©éƒ½æ˜¯æƒ³è¦è¿½æ±‚ä¸»æ’­æƒ³å’Œä¸»æ’­ç»“å©šã€‚ä½†æ˜¯é‚£ä¸ªå¼€å°å·ç›’æˆ‘çš„äººï¼Œè‚¯å®šæ˜¯è¿™æ ·æƒ³çš„ã€‚ä»–è§‰å¾—æˆ‘çœ‹ç”·ä¸»æ’­ï¼Œå°±æ˜¯å–œæ¬¢ä»–ï¼Œå¯¹ä»–æœ‰éåˆ†ä¹‹æƒ³ï¼Œå°±è§‰å¾—æˆ‘çš„ç²‰ä¸å¾ˆå‚»é€¼ï¼Œæƒ³åˆ©ç”¨ä»–ä»¬ä¸€åˆ€æ…æ­»æˆ‘ã€‚
ä»¥åŠæˆ‘è§‰å¾—ï¼Œäººè‚‰è¿™ä¸ªäº‹æƒ…ï¼Œæœ¬æ¥å°±æ˜¯è¿æ³•çš„ï¼Œæˆ‘ä»¬åº”è¯¥ç—›æ–¥è¿™ä¸ªç›’ç‹—ã€‚æˆ‘å’Œæˆ‘çš„ç²‰ä¸éƒ½æ²¡æœ‰åšé”™ä»€ä¹ˆï¼Œåšé”™çš„æ˜¯è¿™ä¸ªç›’ç‹—ã€‚
ç„¶åæˆ‘æ³¨é”€è´¦å·çš„åŸå› ï¼Œç¬¬ä¸€æ˜¯å› ä¸ºæˆ‘æƒ³è®©æˆ‘çš„ç²‰ä¸å®‰å¿ƒï¼Œæˆ‘çœŸçš„ä¸æƒ³å’Œè¿™ä¸ªä¸»æ’­ç»“å©šï¼Œä»–å°±æ˜¯æˆ‘å“¥å“¥ã€æˆ‘å¶åƒã€æˆ‘æƒ³æˆä¸ºçš„äººçš„ä¸€ä¸ªè¿™æ ·çš„å­˜åœ¨ï¼Œè€Œä¸”ï¼Œäººå®¶é‚£ä¹ˆä¼˜ç§€ï¼Œä¹Ÿä¸ä¼šå–œæ¬¢æˆ‘ã€‚
ç¬¬äºŒï¼Œæˆ‘è®¤ä¸ºè¿‡å»çš„äº‹æƒ…ä¸ºä½•çè´µï¼Œçè´µçš„æ˜¯ä½ æ‹¥æœ‰çš„è¿™ä¸€ä¸ªç‹¬ä¸€æ— äºŒçš„ç»å†ï¼Œå®ƒæ˜¯åˆ»åœ¨ä½ çš„éª¨å¤´é‡Œçš„æ˜¯æ°¸è¿œé“­è®°äºå¿ƒçš„ï¼Œæ˜¯ä»»ä½•äººéƒ½æ— æ³•å¤ºèµ°çš„ã€‚è€Œä¸”ï¼Œçè´µçš„ä¸æ˜¯â€œè´¦å·â€è€Œæ˜¯â€œäººâ€ã€‚æˆ‘çš„å¶åƒä¸ä¼šå› ä¸ºæˆ‘æ¢äº†ä¸€ä¸ªè´¦å·å°±ä¸è®¤è¯†æˆ‘ï¼Œå›å¿†æ˜¯ç›¸äº’çš„ï¼Œä»–è®°å¾—æˆ‘çš„æ•…äº‹ï¼Œè®°å¿†å¹¶ä¸ä¼šå› ä¸ºè´¦å·çš„æ¶ˆå¤±è€Œæ¶ˆå¤±ã€‚æˆ‘å–œæ¬¢çš„ä¹Ÿåªæ˜¯ä»–çš„çµé­‚ï¼Œä»–ç§¯æå‘ä¸Šçš„æ€åº¦ï¼Œå’Œæ­£èƒ½é‡çš„ä¸‰è§‚ã€‚
æˆ‘ä»¬å¯¹äºå–œæ¬¢çš„äººï¼Œç»™äºˆæ”¯æŒã€ä¼ è¾¾çˆ±æ„ï¼Œä½†æ˜¯ä¸èƒ½å¤ªè¿‡äºæç«¯ï¼Œå–œæ¬¢ä¸€ä¸ªäººï¼Œå¹¶ä¸ä¸€å®šå°±è¦å¾—åˆ°å¥¹ã€‚åªéœ€è¦çœ‹åˆ°å¥¹åœ¨ä½ èƒ½çœ‹åˆ°çš„åœ°æ–¹è¿‡å¾—å¾ˆå¥½ï¼Œæœ‰å› ä¸ºä½ çš„æ”¯æŒè¶Šæ¥è¶Šå¥½ï¼Œå°±å¥½äº†ã€‚å¶åƒçš„å­˜åœ¨ï¼Œæ›´å¤šçš„æ˜¯ä¼ è¾¾æ¢¦æƒ³ï¼Œä»¥åŠç§¯æçš„å…±é¸£ï¼Œå’Œç²‰ä¸å…±å‹‰ï¼Œå…±åŒè¿›æ­¥ï¼Œè¶Šè¿‡è¶Šå¥½ã€‚
ä¸è¦è®©çˆ±ï¼Œå˜æˆä¸€ç§æ·é”ã€‚
æˆ‘æ°¸è¿œä¹Ÿä¸ä¼šæƒ³è¦å¾—åˆ°æˆ‘çš„å¶åƒï¼Œå› ä¸ºæ˜Ÿæ˜Ÿï¼Œåªéœ€è¦åœ¨æˆ‘çœ‹å¾—è§çš„åœ°æ–¹é—ªé—ªå‘å…‰å°±å¥½äº†ã€‚'''
    # ccid = 42177257
    # dmid = 30370332169732101
    # pprint(get_user_videos(3461565011462803)[0])
    # pprint(get_up_info(3461565011462803))
    # pprint(Crawl.get_space_info(5970160))
    # pprint(get_date_range('https://www.bilibili.com/video/BV1ms4y117ow/?vd_source=58a41ac877c965b4616d2d9f764c219d'))
    # pprint(Crawl.get_space_info(276134779))
    # print(auto_wrap(text))
    # Crawl.get_dm_likes(ccid, dmid)
